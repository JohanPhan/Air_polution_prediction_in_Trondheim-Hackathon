{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process import preprocess\n",
    "import plotly.offline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_path = Path('../data/NILU_Dataset_Trondheim_2014-2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df = pd.read_csv(data_path, index_col=[0], header=[0, 1])\n",
    "    \n",
    "    df.index.name = 'timestamp'\n",
    "    df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45158"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'pred_var': 'Torvet PM10', # Must include station and pollutants name (column name)\n",
    "    'stations': ['Torvet'], # Stations to use in feature extraction\n",
    "    'window': 6,\n",
    "    'load': True,\n",
    "    'freeze1': True,\n",
    "    'freeze2': True,\n",
    "    'freeze3': True,\n",
    "    'freeze4': True,\n",
    "    'load_state': True,\n",
    "    'state': 6, #Train estimate of state* hour ahead, 6,12,24,36,48\n",
    "    'core_features': 5\n",
    "}\n",
    "\n",
    "data = preprocess(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Core:\n\tsize mismatch for FC_6_hour.weight: copying a param with shape torch.Size([48, 640]) from checkpoint, the shape in current model is torch.Size([6, 640]).\n\tsize mismatch for FC_6_hour.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for FC_12_hour.weight: copying a param with shape torch.Size([48, 640]) from checkpoint, the shape in current model is torch.Size([6, 640]).\n\tsize mismatch for FC_12_hour.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for FC_24_hour.weight: copying a param with shape torch.Size([48, 640]) from checkpoint, the shape in current model is torch.Size([6, 640]).\n\tsize mismatch for FC_24_hour.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for FC_36_hour.weight: copying a param with shape torch.Size([48, 640]) from checkpoint, the shape in current model is torch.Size([6, 640]).\n\tsize mismatch for FC_36_hour.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for FC_48_hour.weight: copying a param with shape torch.Size([48, 640]) from checkpoint, the shape in current model is torch.Size([6, 640]).\n\tsize mismatch for FC_48_hour.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for FC_merge_info.weight: copying a param with shape torch.Size([48, 240]) from checkpoint, the shape in current model is torch.Size([6, 30]).\n\tsize mismatch for FC_merge_info.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([6]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fe8287e0a7f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeep_meta_learner\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mMLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/v2019-hackathon/solution/Deep_meta_learner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, data)\u001b[0m\n\u001b[1;32m    276\u001b[0m       \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'load_state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m       \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_train2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m       \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_parameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/v2019-hackathon/solution/Deep_meta_learner.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(model, optimizer, Path)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;31m#model2.load_state_dict(checkpoint['model_state_dict'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;31m#FC.load_state_dict(checkpoint['FC_state_dict'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 771\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Core:\n\tsize mismatch for FC_6_hour.weight: copying a param with shape torch.Size([48, 640]) from checkpoint, the shape in current model is torch.Size([6, 640]).\n\tsize mismatch for FC_6_hour.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for FC_12_hour.weight: copying a param with shape torch.Size([48, 640]) from checkpoint, the shape in current model is torch.Size([6, 640]).\n\tsize mismatch for FC_12_hour.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for FC_24_hour.weight: copying a param with shape torch.Size([48, 640]) from checkpoint, the shape in current model is torch.Size([6, 640]).\n\tsize mismatch for FC_24_hour.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for FC_36_hour.weight: copying a param with shape torch.Size([48, 640]) from checkpoint, the shape in current model is torch.Size([6, 640]).\n\tsize mismatch for FC_36_hour.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for FC_48_hour.weight: copying a param with shape torch.Size([48, 640]) from checkpoint, the shape in current model is torch.Size([6, 640]).\n\tsize mismatch for FC_48_hour.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for FC_merge_info.weight: copying a param with shape torch.Size([48, 240]) from checkpoint, the shape in current model is torch.Size([6, 30]).\n\tsize mismatch for FC_merge_info.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([6])."
     ]
    }
   ],
   "source": [
    "import Deep_meta_learner as MLP\n",
    "MLP.train(config, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X train', data['X_train'].shape)\n",
    "print('y train', data['y_train'].shape)\n",
    "print('X validation', data['X_val'].shape)\n",
    "print('X test', data['X_test'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Multi Output RF | GBM | MLP**\n",
    "\n",
    "_Params are hidden inside each file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SMLP as SMLP\n",
    "SMLP.train(config, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 95.27   R2:-40.67\n"
     ]
    }
   ],
   "source": [
    "smlp_results, rmse, r2 = SMLP.predict(config, data)\n",
    "print('RMSE: {:.2f}   R2:{:.2f}'.format(rmse, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.45   R2:0.11\n"
     ]
    }
   ],
   "source": [
    "import GBM as GBM\n",
    "GBM.train(config, data)\n",
    "gbm_results, rmse, r2 = GBM.predict(config, data)\n",
    "print('RMSE: {:.2f}   R2:{:.2f}'.format(rmse, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RF as RF\n",
    "RF.train(config, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 13.53   R2:0.16\n"
     ]
    }
   ],
   "source": [
    "import RF as RF\n",
    "RF.train(config, data)\n",
    "rf_results, rmse, r2 = RF.predict(config, data)\n",
    "print('RMSE: {:.2f}   R2:{:.2f}'.format(rmse, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 10.67   R2:0.10\n"
     ]
    }
   ],
   "source": [
    "mlp_results, rmse, r2 = MLP.predict(config, data)\n",
    "print('RMSE: {:.2f}   R2:{:.2f}'.format(rmse, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import cloudpickle\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "import cufflinks\n",
    "plotly.tools.set_credentials_file(username='johap', api_key='k6Uj8QR961xiIrJLYYs9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP1 = \"MLP_min feature\" \n",
    "MLP1 = \"MLP_min feature\" \n",
    "MLP1 = \"MLP_min feature\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~johap/142.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gbm_results['RF'] = rf_results['RF']\n",
    "gbm_results['MLP'] = mlp_results['MLP']\n",
    "#gbm_results['SMLP'] = smlp_results['MLP']\n",
    "gbm_results['2019-02-15':].iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
